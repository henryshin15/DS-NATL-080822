{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, PrecisionRecallDisplay, auc, RocCurveDisplay\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Curves - For When ROC Fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, hopefully you've all seen and get ROC-AUC by now. The ROC-AUC is a great tool for assessing your model's overall performance and for finding the right threshold for a clasification model. But!  It does have some limitations. Let's explore that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First thing, getting a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sns.load_dataset('taxis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.color.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, here's a dataset dealing with various taxi rides. Our client is a graphic designer who wants to comparing yellow and green cabs. For some reason, known as 'I need an example', they want to try to predict the color of a cab based on other factors of a cab ride. Since I'm just doing an example to show metrics and I'm not trying to incorperate all the data, I'm going to drop some columns that would take too much preprocessing, just for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns I don't want to deal with. \n",
    "# I don't feel like dealing with the time, and the pickup_zone and dropoff_zone both have a ton of values. \n",
    "data_dropped = data.drop(['pickup', 'dropoff','pickup_zone','dropoff_zone'], axis = 1)\n",
    "\n",
    "# dropping nans\n",
    "data_drop_nan = data_dropped.dropna()\n",
    "\n",
    "# creating data and target dataframes\n",
    "data_X = data_drop_nan.drop('color', axis = 1)\n",
    "target = data_drop_nan.color\n",
    "\n",
    "# I'm going to relabel the target column into binary. This is won't cause data leak issues. \n",
    "# I am leaving scaling and OHE for after the tts of course.\n",
    "target_binary = target.replace(to_replace = ['yellow', 'green'], value = [0,1])\n",
    "\n",
    "# train test split\n",
    "# I'm going to stratify based on target because there's *some* imbalance. \n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, target_binary, stratify = target_binary, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('y train ratio = '+ str(y_train.value_counts()[0]/y_train.value_counts()[1]))\n",
    "print('y test ratio = '+ str(y_test.value_counts()[0]/y_test.value_counts()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio here is around 5.5 to 1, which is fairly imbalanced. Let's see what happens when we run a pretty simple model on it. First, preprocessing.\n",
    "\n",
    "In a bit we're going to cover ColumnTransformer and Pipelines that will make these steps quicker, for now I'm going to fly through this, I just need a simple model so I can show off AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe1 = OneHotEncoder(sparse = False, handle_unknown = 'ignore')\n",
    "scaler1 = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_OHE = ['payment', 'pickup_borough', 'dropoff_borough']\n",
    "columns_for_scaling = ['distance', 'fare', 'tip', 'tolls', 'total']\n",
    "# passengers is weird here, it's possible to think of them as just a numeric feature to be scaled, \n",
    "# but also to see it as an ordinal category. In this case I'm going to not scale it, but in a real project\n",
    "# I might test both versions to see if it changes the model. \n",
    "\n",
    "\n",
    "#Fit the ohe encoder on the categorical data:\n",
    "ohe1.fit(X_train[columns_for_OHE])\n",
    "\n",
    "#Transform the training data\n",
    "encoded_columns = pd.DataFrame(ohe1.transform(X_train[columns_for_OHE]), \n",
    "                               columns = [ohe1.get_feature_names_out()], \n",
    "                               index = X_train.index)\n",
    "\n",
    "#Fit the scaler on the continuous numeric variables\n",
    "scaler1.fit(X_train[columns_for_scaling])\n",
    "\n",
    "#Transform using the scaler\n",
    "scaled_columns = pd.DataFrame(scaler1.transform(X_train[columns_for_scaling]), \n",
    "                              columns = [scaler1.feature_names_in_], \n",
    "                              index = X_train.index)\n",
    "\n",
    "\n",
    "display(encoded_columns.head())\n",
    "display(scaled_columns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the two transformed data frame\n",
    "X_train_encscaled = scaled_columns.join(encoded_columns)\n",
    "\n",
    "# And now adding the one column I didn't encode or scale. \n",
    "# Again, ColumnTransformer and Pipelines makes this much nicer.\n",
    "X_train_encscaled['passengers']= X_train.passengers\n",
    "\n",
    "X_train_encscaled "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally done with that! Let's put this in a model, get an AUC score, and maybe I can talk about Precision-Recall Curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baselinemodel = DummyClassifier()\n",
    "lrmodel1 = LogisticRegression(random_state = 14)\n",
    "\n",
    "baselinemodel.fit(X_train_encscaled, y_train)\n",
    "lrmodel1.fit(X_train_encscaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudopipeline(Xdata, fitenc, fitscaler):\n",
    "    '''\n",
    "    Takes in a test X data set, fit ohe and fit scaler.\n",
    "    Returns a properly set up X_test to feed to fit models.\n",
    "    '''\n",
    "    pipe_encoded_columns = pd.DataFrame(fitenc.transform(Xdata[columns_for_OHE]), \n",
    "                               columns = [fitenc.get_feature_names_out()], \n",
    "                               index = Xdata.index)\n",
    "    pipe_scaled_columns = pd.DataFrame(fitscaler.transform(Xdata[columns_for_scaling]), \n",
    "                              columns = [fitscaler.feature_names_in_], \n",
    "                              index = Xdata.index)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    X_df_encscaled = pipe_scaled_columns.join(pipe_encoded_columns)\n",
    "    X_df_encscaled['passengers']= Xdata.passengers\n",
    "    return X_df_encscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this example, I will now drop many of the 'green' taxis from the y test data to simulate a very imbalanced class problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_green = y_test.loc[y_test== 1].sample(frac=.1,  random_state=14)\n",
    "#X_test[]\n",
    "print(len(y_green))\n",
    "y_yellow = y_test.loc[y_test == 0]\n",
    "y_yellow\n",
    "new_y_test = pd.DataFrame(pd.concat([y_green, y_yellow]))\n",
    "print(len(new_y_test))\n",
    "new_test_df = new_y_test.join(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you get here, don't run cells above this, we're using new values for y_test and X_test\n",
    "# Unless you run the whole thing back from the top\n",
    "\n",
    "y_test = new_test_df.color\n",
    "\n",
    "X_test = new_test_df.drop(labels = 'color', axis = 1)\n",
    "\n",
    "#I'll run my X_test data into my pseudopipeline\n",
    "\n",
    "X_test_enscaled = pseudopipeline(X_test, ohe1, scaler1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating predictions with default threshold\n",
    "y_pred_dummy = baselinemodel.predict(X_test_enscaled)\n",
    "y_pred_1 = lrmodel1.predict(X_test_enscaled)\n",
    "\n",
    "\n",
    "\n",
    "#Generating my predictions as probabilities\n",
    "y_pred_proba_dummy = baselinemodel.predict_proba(X_test_enscaled)\n",
    "y_pred_proba_1 = lrmodel1.predict_proba(X_test_enscaled)\n",
    "\n",
    "#\n",
    "def confusiondisplay(y_true, y_pred):\n",
    "    cm = ConfusionMatrixDisplay.from_predictions(y_true, y_pred)\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusiondisplay(y_test, y_pred_dummy)\n",
    "print('DummyConfusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 1 Confusion Matrix')\n",
    "confusiondisplay(y_test, y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotrocauc(y_true, y_pred_probs, model_name):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "    display.plot()\n",
    "    auc_score = roc_auc_score(y_true, y_pred_probs)\n",
    "    print('ROC - AUC score for ' + model_name + ' is ' + str(auc_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotrocauc(y_test, y_pred_proba_1[ :,1], 'model1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotrocauc(y_test, y_pred_proba_dummy[ :,1], 'Dummy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so what's the deal with this PR curve?  Where a ROC curve charts true and false positive *rates*, the Precision-Recall curve charts... precision and recall. \n",
    "\n",
    "Remember that Precision is the same as the number of true positives divided by the total number of both true and false positives. This is the same as the 'TPR' in the ROC-AUC metric.  A high precision means that you are rarely missing positive cases. As we've seen, you can get more of the true positive by trading off, with a corresponding growth in false negatives. \n",
    "\n",
    "So the real difference with PR-AUC and ROC-AUC is Recall, which divides your total number of true positive guesses by the overall number of positive guesses. To increase recall, you need to weed out false negatives. \n",
    "\n",
    "|                   | Guessed True  | Guessed False  |\n",
    "| --------------    | --------------|--------------- |\n",
    "| **Actually True** |        TP     |        FN      |\n",
    "| **Actually False**|        FP     |        TN      |\n",
    "\n",
    "\n",
    "Precision Recall can be a more useful metric in cases of imbalanced datasets. An ROC-AUC on an imbalanced dataset might 'get away with' an inflated TPR and FPR if the imbalance leads to a relatively 'good' rate of true negatives.\n",
    "\n",
    "\n",
    "Consider a case like this:\n",
    "\n",
    "TP=9, FN=1, FP=90, TN=900 \n",
    "\n",
    "|                    | Guessed Green     | Guessed Yellow     |\n",
    "| --------------     | ------------------|------------------- |\n",
    "| **Actually Green** |        9          |        1           |\n",
    "| **Actually Yellow**|       90          |       900          |\n",
    "\n",
    "Now, how well do we think this model worked? Well, if we're trying to find Greens, we did pretty well. And while we had a lot of yellows in the green guesses, we were fairly discriminating. Mostly, we were able to class the Yellows into the right column.\n",
    "\n",
    "\n",
    "Let's see how this would reflect in our ROC-AUC curve at whatever threshold we're using. In this case our FPR is **FP / (FP + TN)** or 1/(1+900). The large amount of 'True Yellows' in the sample lead this to have a 'good' FPR score, maybe better than we might feel is justified. The large number of 'Actually Yellow' guesses will 'stick' the curve to the left. On the other hand, the TPR (**TP/(TP+FP)**)is less good at (9/9+90). That's obviously not good, but it is skewed by the exaggerated number of 'Actually Yellow' responses in the dataset. Although we could use the ROC to find a good threshold with our model, the ROC-AUC score in general will be skewed by this class imbalance.\n",
    "\n",
    "Let's compare that to the Precision - Recall metric. The Precision is the same as the TPR **(TP/(TP+FP)**), but Recall is **TP / (TP + FN)** or 9/(9+1), or 90%. That's not bad. The PR-Curve 'looks at' the upper left, upper right, and lower left cells, but doesn't look at the lower right. In other words, **TN** is not accounted for in the PR-AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotprauc(fitestimator, xtest, y_true, model_name, y_pred_probs):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_probs)\n",
    "    PrecisionRecallDisplay.from_estimator(estimator = fitestimator, X = xtest, y = y_true, name = model_name)\n",
    "    auc_score = auc(recall, precision)\n",
    "    print('PR - AUC score for ' + model_name + ' is ' + str(auc_score))\n",
    "    #disp.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotprauc(lrmodel1, X_test_enscaled, y_test,  'Model 1', y_pred_proba_1[ :,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotprauc(baselinemodel, X_test_enscaled, y_test,'Dummy', y_pred_proba_dummy[ :,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
